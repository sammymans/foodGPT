{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efcDiWasulic"
   },
   "source": [
    "# **Recommendation System Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kklEMiEVY80"
   },
   "source": [
    "## *Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install statistics\n",
    "!pip install sklearn\n",
    "!pip install scipy\n",
    "!pip install seaborn\n",
    "!pip install matplotlib\n",
    "!pip install tqdm\n",
    "!pip install surprise\n",
    "!pip install lightfm\n",
    "!pip install gensim\n",
    "!pip install pickle\n",
    "!pip install ./scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TL2xzlIivVHQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import pickle\n",
    "import gensim\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.decomposition import NMF, MiniBatchNMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from surprise import Dataset, Reader\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from smart_substitution_model import df_recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkNTFP_5S8mK"
   },
   "source": [
    "## **Data Set: User Interactions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOSHFBz9cDx5"
   },
   "source": [
    "The provided dataset contains a split for the test, train, and validation sets. Data contains dated user interactions with recipes found on food.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ePCsNvlXTvPC",
    "outputId": "da761976-e3ff-411f-e143-e6f353b9ee8d"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/interactions_test.csv')\n",
    "train = pd.read_csv('data/interactions_train.csv')\n",
    "validation = pd.read_csv('data/interactions_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "r4Vey1lOVx3H",
    "outputId": "d1c711cc-1cff-4306-a717-0f7583bdeb31"
   },
   "outputs": [],
   "source": [
    "dataframes = [test, train, validation]\n",
    "\n",
    "for df in dataframes:\n",
    "    df.insert(0, 'u', df.pop('u'))\n",
    "    df.insert(1, 'i', df.pop('i'))\n",
    "    df.drop(['user_id', 'date', 'recipe_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.rename(columns={'u': 'user_id'})\n",
    "train = train.rename(columns={'i': 'recipe_id'})\n",
    "\n",
    "test = test.rename(columns={'u': 'user_id'})\n",
    "test = test.rename(columns={'i': 'recipe_id'})\n",
    "\n",
    "validation = validation.rename(columns={'u': 'user_id'})\n",
    "validation = validation.rename(columns={'i': 'recipe_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([test, train, validation], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce sparseness of the rating matrix, users with less than a specified amount of reviews was removed from the dataset. New train, test, validation datasets were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_counts = combined['user_id'].value_counts().sort_index()\n",
    "bad_users = []\n",
    "\n",
    "for user, val in user_counts.items():\n",
    "  if val < 500:\n",
    "    bad_users.append(user)\n",
    "  \n",
    "print(str(len(bad_users)) + \" --> new dataset: \" + str(25075 - len(bad_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed = combined[~combined['user_id'].isin(bad_users)].dropna()\n",
    "print(trimmed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(trimmed, test_size=0.15, random_state=36)\n",
    "train, validation = train_test_split(train, test_size=0.15, random_state=36)\n",
    "\n",
    "print('total size: ' + str(train.shape[0] + test.shape[0] + validation.shape[0]))\n",
    "print('train: ' + str(train.shape))\n",
    "print('test: ' + str(test.shape))\n",
    "print('validation: ' + str(validation.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Rating Matricies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "rating_matrix = pd.pivot_table(train, values='rating', index='user_id', columns='recipe_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "rating_matrix_v = pd.pivot_table(validation, values='rating', index='user_id', columns='recipe_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cols = set(rating_matrix.columns)\n",
    "validation_cols = set(rating_matrix_v.columns)\n",
    "common = training_cols.intersection(validation_cols)\n",
    "common = list(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix = rating_matrix.reindex(columns=common, fill_value=0)\n",
    "rating_matrix_v = rating_matrix_v.reindex(columns=common, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix = rating_matrix.reindex(sorted(rating_matrix.columns), axis=1)\n",
    "rating_matrix_v = rating_matrix_v.reindex(sorted(rating_matrix_v.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix_array = rating_matrix.values\n",
    "rating_matrix_array_v = rating_matrix_v.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Rating Matricies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(rating_matrix_array[:250], cmap='YlGnBu')\n",
    "plt.title('Rating Matrix - Train')\n",
    "plt.xlabel('Recipe')\n",
    "plt.ylabel('User')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(rating_matrix_array_v[:250], cmap='YlGnBu')\n",
    "plt.title('Rating Matrix - Validation')\n",
    "plt.xlabel('Recipe')\n",
    "plt.ylabel('User')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_components=20, alpha_W=0.0, alpha_H='same', l1_ratio=0.0):\n",
    "    init = 'random'\n",
    "    solver = 'mu'\n",
    "    beta_loss = 'frobenius'\n",
    "    tol = 1e-4\n",
    "    max_iter = 1000 \n",
    "    random_state = 10\n",
    "    verbose = 0\n",
    "    shuffle = False\n",
    "    model = NMF(n_components=n_components, init=init, solver=solver, beta_loss=beta_loss, tol=tol, max_iter=max_iter, random_state=random_state, alpha_W=alpha_W, alpha_H=alpha_H, l1_ratio=l1_ratio, verbose=verbose, shuffle=shuffle)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model):\n",
    "    user_features = model.fit_transform(rating_matrix_array)\n",
    "    recipe_features = model.components_\n",
    "    val_set_transformed = model.transform(rating_matrix_array_v)\n",
    "    predicted_val_ratings = np.dot(val_set_transformed, recipe_features)\n",
    "    rating_matrix_v_masked = np.invert(np.isnan(rating_matrix_array_v)).astype(int)\n",
    "    rmse = np.sqrt(mean_squared_error(rating_matrix_v_masked, predicted_val_ratings))\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default model\n",
    "n_components = 2\n",
    "init = 'random'\n",
    "solver = 'mu'\n",
    "beta_loss = 'frobenius'\n",
    "tol = 1e-4\n",
    "max_iter = 1000 \n",
    "random_state = 10 \n",
    "alpha_W = 0.0 \n",
    "alpha_H = 'same'\n",
    "l1_ratio = 0.0\n",
    "verbose = 0\n",
    "shuffle = False\n",
    "\n",
    "model = NMF(n_components=n_components, init=init, solver=solver, beta_loss=beta_loss, tol=tol, max_iter=max_iter, random_state=random_state, alpha_W=alpha_W, alpha_H=alpha_H, l1_ratio=l1_ratio, verbose=verbose, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "user_features = model.fit_transform(rating_matrix_array)\n",
    "recipe_features = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "val_set_transformed = model.transform(rating_matrix_array_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predicted_val_ratings = np.dot(val_set_transformed, recipe_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make mask\n",
    "rating_matrix_v_masked = np.invert(np.isnan(rating_matrix_array_v)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "sns.heatmap(predicted_val_ratings[:100], cmap='YlGnBu', vmin=0, vmax=5)\n",
    "plt.title('Rating Matrix: Predictions')\n",
    "plt.xlabel('Recipe')\n",
    "plt.ylabel('User')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determination of Optimal Parameters + Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, GridSearchCV was employed, as well as observing the effect of fitting the model to different parameters over several epochs. Variables experimented with were:\n",
    "- n_components\n",
    "- alpha_H\n",
    "- alpha_W\n",
    "- l1_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "rmse = np.sqrt(mean_squared_error(rating_matrix_v_masked, predicted_val_ratings))\n",
    "print('Calculated RMSE Value:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing n_components\n",
    "rmse_list = []\n",
    "reconstructed_errors = []\n",
    "\n",
    "for num_comp in range(1,30):\n",
    "    model = create_model(n_components=num_comp)\n",
    "    rmse_val = run_model(model)\n",
    "    rmse_list.append(rmse_val)\n",
    "    reconstructed_errors.append(model.reconstruction_err_)\n",
    "    print('finished: #' + str(num_comp))\n",
    "    \n",
    "    \n",
    "plt.plot(rmse_list)\n",
    "plt.xlabel('#')\n",
    "plt.ylabel('rmse')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(reconstructed_errors)\n",
    "plt.xlabel('#')\n",
    "plt.ylabel('reconst')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing alpha_H\n",
    "rmse_list = []\n",
    "reconstructed_errors = []\n",
    "\n",
    "new_alpha_H = 0.001/40\n",
    "for i in range(25):\n",
    "    new_alpha_H *= 40 \n",
    "    model = create_model(alpha_H=new_alpha_H)\n",
    "    rmse_val = run_model(model)\n",
    "    rmse_list.append(rmse_val)\n",
    "    reconstructed_errors.append(model.reconstruction_err_)\n",
    "    print('finished: #' + str(i))\n",
    "    \n",
    "    \n",
    "plt.plot(rmse_list)\n",
    "plt.xlabel('#')\n",
    "plt.ylabel('rmse')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(reconstructed_errors)\n",
    "plt.xlabel('#')\n",
    "plt.ylabel('reconst')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing alpha_W\n",
    "rmse_list = []\n",
    "reconstructed_errors = []\n",
    "\n",
    "new_alpha_W = 0.001/40\n",
    "for i in range(25):\n",
    "    new_alpha_W *= 40 \n",
    "    model = create_model(alpha_W=new_alpha_W)\n",
    "    rmse_val = run_model(model)\n",
    "    rmse_list.append(rmse_val)\n",
    "    reconstructed_errors.append(model.reconstruction_err_)\n",
    "    print('finished: #' + str(i))\n",
    "    \n",
    "    \n",
    "plt.plot(rmse_list)\n",
    "plt.xlabel('#')\n",
    "plt.ylabel('rmse')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(reconstructed_errors)\n",
    "plt.xlabel('#')\n",
    "plt.ylabel('reconst')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing l1_ratio\n",
    "rmse_list = []\n",
    "reconstructed_errors = []\n",
    "\n",
    "new_l1_ratio = -0.05\n",
    "for i in range(20):\n",
    "    new_l1_ratio += 0.05 \n",
    "    model = create_model(l1_ratio=new_l1_ratio)\n",
    "    rmse_val = run_model(model)\n",
    "    rmse_list.append(rmse_val)\n",
    "    reconstructed_errors.append(model.reconstruction_err_)\n",
    "    print('finished: #' + str(i))\n",
    "    \n",
    "    \n",
    "plt.plot(rmse_list)\n",
    "plt.xlabel('#')\n",
    "plt.ylabel('rmse')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(reconstructed_errors)\n",
    "plt.xlabel('#')\n",
    "plt.ylabel('reconst')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "model_params = {'n_components': [15,16,17,18,19,20,21,22,23,24,25],\n",
    "                'beta_loss': ['frobenius', 'kullback-leibler'],\n",
    "                'alpha_W': [0.001, 0.01, 0.1, 1],\n",
    "                'alpha_H': [0.001, 0.01, 0.1, 1],\n",
    "                'l1_ratio': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\n",
    "\n",
    "n_components = 2\n",
    "init = 'random'\n",
    "solver = 'mu'\n",
    "max_iter = 1000 \n",
    "random_state = 10\n",
    "nmf_model = NMF(init=init, solver=solver, beta_loss=beta_loss, max_iter=max_iter, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(nmf_model, model_params, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(rating_matrix_v_masked, predicted_val_ratings)\n",
    "\n",
    "print(grid_search.best_estimator_)\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", -grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following data cleaning and finding optimal parameters for the NMF model, the recommendation system needs to be merged with the smart substitution algorithm. We can use `from recipe_similarity import df_recipe` to retrieve the dataframe with similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rating_matrix = pd.DataFrame(data=predicted_val_ratings, index=rating_matrix.index, columns=rating_matrix.columns)\n",
    "final_predictions_df = predict_rating_matrix.stack().reset_index()\n",
    "final_predictions_df.columns = ['user_id', 'recipe_id', 'rating']\n",
    "final_predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_id = int(input('Input a number for a specified user: '))\n",
    "df_user = final_predictions_df[final_predictions_df['user_id'] == demo_id][['recipe_id', 'rating']]\n",
    "df_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recipe = df_recipe.reset_index()\n",
    "df_recipe = df_recipe.rename(columns={'index':'recipe_id'})\n",
    "df_recipe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rec = pd.merge(df_user, df_recipe, on='recipe_id')\n",
    "final_rec = final_rec.drop(columns=['ids'])\n",
    "final_rec.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rec[\"rating\"] = final_rec[\"rating\"] / 5\n",
    "final_rec[\"average\"] = (final_rec[\"rating\"] + final_rec[\"score\"]) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Top 10 Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two models were combined by using the user predicted ratings from the NMF model and the similarity scores from the smart substitution model. We can normalize the rating to get a number from 0-1, then average the result of the rating and the similarity score to obtain a cumulative score. We can then use this number to determine what the top recommended recipes are based on the inputted user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rec = final_rec.sort_values(by=\"average\", ascending=False)\n",
    "final_rec.head(10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
